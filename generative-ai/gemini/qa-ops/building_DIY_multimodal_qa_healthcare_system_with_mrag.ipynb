{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDsTUvKjwHBW"
   },
   "source": [
    "# Building a DIY Healthcare Multimodal Question Answering System with Vertex AI (A Beginner's Guide - Multimodal RAG)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_healthcare_system_with_mrag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fqa-ops%2Fbuilding_DIY_multimodal_qa_healthcare_system_with_mrag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_healthcare_system_with_mrag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/qa-ops/building_DIY_multimodal_qa_healthcare_system_with_mrag.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nATdRbiafisv"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp), [Ayo Adedeji]()  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Welcome to this hands-on workshop where we explore the cutting-edge field of multimodal Retrieval-Augmented Generation (RAG). In this session, we'll focus on utilizing both textual and visual data to perform complex question answering (Q&A) over CVS and Medicare documents. This approach not only enriches the interaction with the data but also enhances the decision-making process by leveraging a broader spectrum of information.\n",
    "\n",
    "## Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation has revolutionized the way Large Language Models (LLMs) interact with information by enabling them to access external data. This not only enriches their knowledge base but also grounds their responses, significantly reducing the likelihood of generating inaccurate information, commonly referred to as \"hallucinations.\"\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cvs-era-of-gemini-workshop/images/multimodal-rag.png\" alt=\"Multimodal RAG Diagram\" height=\"500\">\n",
    "\n",
    "## Understanding Gemini: A Multimodal AI by DeepMind\n",
    "\n",
    "Gemini is a suite of generative AI models developed by Google DeepMind tailored for multimodal applications. With Gemini, you can process and analyze data that includes text, images, audio, and even videos. The API provides seamless access to various versions of the Gemini models, each designed for specific capabilities:\n",
    "\n",
    "    Gemini 1.0 Pro Vision: Supports multimodal prompts. You can include text, images, and video in your prompt requests and get text or code responses.\n",
    "    Gemini 1.0 Pro: Designed to handle natural language tasks, multiturn text and code chat, and code generation.\n",
    "    Gemini 1.5 Pro: Created to be multimodal (text, images, audio, PDFs, code, videos) and to scale across a wide range of tasks with up to 1M input tokens.\n",
    "\n",
    "## Advantages of Multimodal RAG over Text-Based RAG\n",
    "\n",
    "Multimodal RAG extends the capabilities of text-only RAG by integrating visual processing, which offers several advantages:\n",
    "\n",
    "    Enhanced Knowledge Access: By analyzing both text and visual content, the model taps into a richer knowledge base, providing more comprehensive insights.\n",
    "    Improved Reasoning Capabilities: Visual cues enable the model to make more informed inferences, improving accuracy and relevance in responses.\n",
    "\n",
    "## Objectives and Learning Outcomes\n",
    "\n",
    "In this workshop, you will learn to build a robust document search engine using multimodal RAG. Here are the key steps we will cover:\n",
    "\n",
    "    Document Processing: Extract and store metadata from documents containing text and images.\n",
    "    Embedding Generation: Produce text and multimodal embeddings to facilitate efficient search capabilities.\n",
    "    Metadata Searching:\n",
    "        Use text queries to locate similar text or images within the document metadata.\n",
    "        Use image queries to find related images.\n",
    "        Employ text queries to search for contextual answers using both text and image data.\n",
    "\n",
    "## Hands-on Examples\n",
    "\n",
    "Throughout this workshop, you'll engage in practical exercises that illustrate how to:\n",
    "\n",
    "    Construct a Multimedia Metadata Repository: This repository will serve as the backbone of your document search engine, enabling sophisticated search, comparison, and reasoning across various types of data.\n",
    "    Use Gemini API for Multimodal Queries: Learn how to send mixed media prompts to Gemini models and interpret the outputs effectively.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this workshop, you'll have a foundational understanding of how to implement and utilize multimodal RAG for enhanced data interaction and retrieval. This knowledge will empower you to handle more complex data sets and contribute to advancements in the field of generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnpYxfesh2rI"
   },
   "source": [
    "## Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXJpXzKrh2rJ"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtsU9Bw9h2rL"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpYEyLsOh2rL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vKZZoEh2rL"
   },
   "source": [
    "### Define Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJqZ76rJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D48gUW5-h2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich import print as rich_print\n",
    "from rich.markdown import Markdown as rich_Markdown\n",
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-TX_R_xh2rM"
   },
   "source": [
    "### Load the Gemini 1.0 Pro, Gemini 1.5 Pro and Gemini 1.0 Pro Vision model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSfMfpSudmYp"
   },
   "source": [
    "Learn more about each models and their differences: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts)\n",
    "\n",
    "Learn about the quotas: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvMwSRJJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate text model with appropriate name and version\n",
    "text_model = GenerativeModel(\"gemini-1.0-pro\")  # works with text, code\n",
    "\n",
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "multimodal_model_15 = GenerativeModel(\n",
    "    \"gemini-1.5-pro-preview-0409\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context\n",
    "\n",
    "multimodal_model_10 = GenerativeModel(\n",
    "    \"gemini-1.0-pro-vision-001\"\n",
    ")  # works with text, code, video(without audio) and images with 16k input context\n",
    "\n",
    "# Load text embedding model from pre-trained source\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "\n",
    "# Load multimodal embedding model from pre-trained source\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding\"\n",
    ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9FLk2ExFLMD"
   },
   "source": [
    "### Download custom Python modules and utilities\n",
    "\n",
    "The cell below will download some helper functions needed for this notebook, to improve readability. You can also view the code (`multimodal_qa_with_rag_utils`) [directly](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/utils/multimodal_qa_with_rag_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSWf6IN_FLqs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/utils/multimodal_qa_with_rag_utils.py\"\n",
    "folder_name = \"utils\"\n",
    "filename = \"multimodal_qa_with_rag_utils.py\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Construct the full file path within the folder\n",
    "file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"Downloaded {filename} to {folder_name} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7bKCQMFT7JT"
   },
   "source": [
    "#### Get documents and images from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwbL89zcY39N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download documents and images used in this notebook\n",
    "!gsutil -m rsync -r gs://cvs-era-of-gemini-workshop/materials/multimodal-rag .\n",
    "print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps1G-cCfpibN"
   },
   "source": [
    "## Building metadata of documents containing text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqLsy3iZ5t-R"
   },
   "source": [
    "### The data\n",
    "\n",
    "The source data that you will use in this notebook consists of a collection of CVS benefit plan and Medicare benefit documents. These documents provide detailed information about over-the-counter costs, services covered, and related healthcare benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiIYG2d_yrjx"
   },
   "source": [
    "### Import helper functions to build metadata\n",
    "\n",
    "Before building the multimodal RAG system, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which will is required to perform similarity search when quering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1eiSwtuysKN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.multimodal_qa_with_rag_utils import (\n",
    "    get_document_metadata,\n",
    "    set_global_variable,\n",
    ")\n",
    "\n",
    "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
    "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOAkYN0KlSL"
   },
   "source": [
    "### Extract and store metadata of text and images from a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9hBPPWs5CMd"
   },
   "source": [
    "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py) directly.\n",
    "\n",
    "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnKru0sBh2rN"
   },
   "source": [
    "At the next step, you will use the function to extract and store metadata of text and images froma document. Please note that the following cell may take a few minutes to complete:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTBVPx0Kfis3"
   },
   "source": [
    "Note:\n",
    "\n",
    "The current implementation works best:\n",
    "\n",
    "1) if your documents are a combination of text and images.\n",
    "2) if the tables in your documents are available as images.\n",
    "3) if the images in the document don't require too much context.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "1) If you want to run this on text-only documents, use normal RAG\n",
    "2) If your documents contain particular domain knowledge, pass that information in the prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8hE0tWD-lf8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the PDF folder with multiple PDF\n",
    "\n",
    "# pdf_folder_path = \"/content/data/\" # if running in Google Colab/Colab Enterprise\n",
    "pdf_folder_path = \"data/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "# If it's a table, extract all elements of the table.\n",
    "# If it's a graph, explain the findings in the graph.\n",
    "# Do not include any numbers that are not mentioned in the image.\n",
    "# \"\"\"\n",
    "\n",
    "image_description_prompt = \"\"\"You are a technical image analysis expert. You will be provided with various types of images extracted from documents like research papers, technical blogs, and more.\n",
    "Your task is to generate concise, accurate descriptions of the images without adding any information you are not confident about.\n",
    "Focus on capturing the key details, trends, or relationships depicted in the image.\n",
    "\n",
    "Important Guidelines:\n",
    "* Prioritize accuracy:  If you are uncertain about any detail, state \"Unknown\" or \"Not visible\" instead of guessing.\n",
    "* Avoid hallucinations: Do not add information that is not directly supported by the image.\n",
    "* Be specific: Use precise language to describe shapes, colors, textures, and any interactions depicted.\n",
    "* Consider context: If the image is a screenshot or contains text, incorporate that information into your description.\n",
    "\"\"\"\n",
    "\n",
    "# Extract text and image metadata from the PDF document\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model_10,  # we are passing gemini 1.0 pro vision model\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
    "    # sleep_time_after_page = 5,\n",
    "    add_sleep_after_document=True,  # Uncomment this if you are running into API quota issues\n",
    "    sleep_time_after_document=5,\n",
    "    # generation_config = # see next cell\n",
    "    # safety_settings =  # see next cell\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI2Gm4gbfis3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Parameters for Gemini API call.\n",
    "# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
    "\n",
    "# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)\n",
    "\n",
    "# # Set the safety settings if Gemini is blocking your content or you are facing \"ValueError(\"Content has no parts\")\" error or \"Exception occured\" in your data.\n",
    "# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
    "\n",
    "# safety_settings = {\n",
    "#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   }\n",
    "\n",
    "# # You can also pass parameters and safety_setting to \"get_gemini_response\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBBoEXwh2rN"
   },
   "source": [
    "#### Inspect the processed text metadata\n",
    "\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of text metadata, including:\n",
    "\n",
    "- **text**: the original text from the page\n",
    "- **text_embedding_page**: the embedding of the original text from the page\n",
    "- **chunk_text**: the original text divided into smaller chunks\n",
    "- **chunk_number**: the index of each text chunk\n",
    "- **text_embedding_chunk**: the embedding of each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t3AIGFar8Mo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjIQYI3mh2rO"
   },
   "source": [
    "#### Inspect the processed image metadata\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
    "* **img_desc**: Gemini-generated textual description of the image.\n",
    "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
    "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
    "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkHtAYIK-y-q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7c-g7V0zNTE"
   },
   "source": [
    "### Import the helper functions to implement RAG\n",
    "\n",
    "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
    "\n",
    "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
    "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
    "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
    "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` fuction.\n",
    "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
    "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09FLv0PzN6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.multimodal_qa_with_rag_utils import (\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_text_citation,\n",
    "    get_similar_image_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    get_gemini_response,\n",
    "    display_images,\n",
    "    get_answer_from_qa_system,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9jGEj6DY1Rj"
   },
   "source": [
    "Before implementing a multimodal RAG, let's take a step back and explore what you can achieve with just text or image embeddings alone. It will help to set the foundation for implementing a multimodal RAG, which you will be doing in the later part of the notebook. You can also use these essential elements together to build applications for multimodal use cases for extracting meaningful information from the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHuLlEvSKFWt"
   },
   "source": [
    "## Text Search\n",
    "\n",
    "Let's start the search with a simple question and see if the simple text search using text embeddings can answer it. The expected answer is to show the value of basic and diluted net income per share of Google for different share types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mrFVhtCut7t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = \"When can I receive my over-the-counter quarterly benefit?\" # Answer present only in text\n",
    "\n",
    "query = \"What is the price of Fexofenadine tablets?\"  # Answer present only in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWw7-AIar-S8"
   },
   "source": [
    "### Search similar text with text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEzP6Yyv7N-G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
    "matching_results_text = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=3,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "# Print the matched text citations\n",
    "print_text_to_text_citation(\n",
    "    matching_results_text, print_top=True, chunk_text=True\n",
    ")  # print_top=False to see all text matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5uT8mBAfitB"
   },
   "source": [
    "In this exercise, you'll notice that the high-scoring match initially seems to contain the information we need, but a detailed review reveals it lacks specific pricing details for Fexofenadine tablets. This omission occurs because the pricing information is presented in an image format within the document, not as searchable text. Consequently, without the ability to process and interpret image data, critical details like this could be overlooked.\n",
    "\n",
    "To address this challenge, let’s input the relevant sections of the document into the Gemini 1.0 Pro model and see if it can integrate information from both the text and image data across the document. This approach exemplifies a basic multimodal RAG implementation, where the model considers multiple data types to provide a more complete answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbt3lNwFZwpk"
   },
   "source": [
    "### Get answer with text-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrdBcpHOZyyZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All relevant text chunk found across documents based on user query\n",
    "context = \"\\n\".join(\n",
    "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"Answer the question with the given context.\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fws51E5QZ0hW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FOB39SoZ05R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.5 Pro\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=prompt,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ6M0TehZ4fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.0 Pro\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        text_model,\n",
    "        model_input=prompt,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=0.4),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8ux39zFqyLh"
   },
   "source": [
    "You can expect a response like the one below:\n",
    "\n",
    "*\"I'm sorry, but the price of Fexofenadine tablets is not listed in the provided context.\"*\n",
    "\n",
    "This outcome aligns with our previous discussions. None of the text sections contain the pricing information you're looking for, primarily because it is presented in image form within the document, rather than as text. To tackle this issue, let's explore how we can effectively utilize the capabilities of Gemini 1.0 Pro Vision along with Multimodal Embeddings to extract and interpret the data embedded in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXm271jdD-Rl"
   },
   "source": [
    "### Search similar images with text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPxwfyVrr9-G"
   },
   "source": [
    "Since a plain text search didn't yield the desired results, and the information may be visually represented in a table or another image format, you will leverage the multimodal capabilities of the Gemini 1.0 Pro Vision model for this task. The goal is to find an image that correlates with your text query about the pricing information. Additionally, you can print the citations to validate the accuracy of the retrieved images.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sRFH6tJlpXQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the price of Fexofenadine tablets?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knj4qQ4xni24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding\n",
    "    image_emb=False,  # Use text embedding instead of image embedding\n",
    "    top_n=5,\n",
    "    embedding_size=1408,\n",
    ")\n",
    "\n",
    "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Display the top matching image\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image[0][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnFdFkWEtYrF"
   },
   "source": [
    "\"Bingo! It found exactly what you were looking for. You wanted the details on the pricing for Fexofenadine tablets, and guess what? This image fits the bill perfectly thanks to its descriptive metadata used by Gemini.\n",
    "\n",
    "You can also submit the image along with its description to the Gemini 1.0 Pro Vision model and receive the detailed pricing information as a JSON response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnQat6JyfitD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "instruction = f\"\"\"Answer the question and explain results with the given Image:\n",
    "Question: {query}\n",
    "Image:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the model input\n",
    "model_input = [\n",
    "    instruction,\n",
    "    # passing all matched images to Gemini\n",
    "    \"Image:\",\n",
    "    matching_results_image[0][\"image_object\"],\n",
    "    \"Description:\",\n",
    "    matching_results_image[0][\"image_description\"],\n",
    "]\n",
    "\n",
    "# Generate Gemini response with streaming output\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,  # we are passing Gemini 1.5 Pro\n",
    "        model_input=model_input,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CADz3JMifitD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## you can check the citations to probe further.\n",
    "## check the \"image description:\" which is a description extracted through gemini which helped search our query.\n",
    "Markdown(print_text_to_image_citation(matching_results_image, print_top=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDd9rE4NrRod"
   },
   "source": [
    "## Image Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJL6ElyEy4mc"
   },
   "source": [
    "### Search similar image with image input [using multimodal image embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReKjHleFxUu9"
   },
   "source": [
    "Imagine using an image as your search query instead of text. For instance, you have a table detailing the cost of revenue for two years and you want to find other images that resemble it, whether they are in the same document or across multiple documents.\n",
    "\n",
    "Think of it as navigating with a visual map rather than a written address. It's a unique way to request, \"Show me more like this.\" Instead of typing out \"cost of revenue 2020-2021 table,\" you simply present a picture of that table and say, \"Find me more like this.\"\n",
    "\n",
    "For demonstration purposes in this exercise, we will focus on finding similar images within a single document—specifically, images that depict over-the-counter labels or something similar. However, this method can be expanded to locate relevant images across multiple documents, showcasing the scalability of this visual search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJhhS5eZw7QI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can find a similar image as per the images you have in the metadata.\n",
    "# In this case, you have an over-the-counter label and you would like to find similar labels in the documents.\n",
    "image_query_path = \"images/Catalog Wellcare.pdf_image_3_3_22.jpeg\"\n",
    "\n",
    "# Print a message indicating the input image\n",
    "print(\"***Input image from user:***\")\n",
    "\n",
    "# Display the input image\n",
    "Image.load_from_file(image_query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zBTtGChTmrd"
   },
   "source": [
    "You expect to find tables (as images) that are similar in terms of \"Other/Total cost of revenues.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZcU7vZC-8vr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for Similar Images Based on Input Image and Image Embedding\n",
    "\n",
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,  # Use query text for additional filtering (optional)\n",
    "    column_name=\"mm_embedding_from_img_only\",  # Use image embedding for similarity calculation\n",
    "    image_emb=True,\n",
    "    image_query_path=image_query_path,  # Use input image for similarity calculation\n",
    "    top_n=3,  # Retrieve top 3 matching images\n",
    "    embedding_size=1408,  # Use embedding size of 1408\n",
    ")\n",
    "\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Display the Top Matching Image\n",
    "display(\n",
    "    matching_results_image[0][\"image_object\"]\n",
    ")  # Display the top matching image object (Pillow Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhT17rke15XY"
   },
   "source": [
    "The search successfully identified a label that closely resembles the one provided, which lists pricing and details for various over-the-counter allergy relief medications. More importantly, both labels feature the \"OTCH Eligible\" tag followed by a SKU number, which is crucial for matching products to your health plan's catalog.\n",
    "\n",
    "You can also view the citation to understand how the match was determined and to verify the accuracy of the information retrieved from the image data. This process demonstrates the capability of multimodal search to not only recognize similar visual patterns but also to contextualize the information within your specific healthcare framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksXQoezweg0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display citation details for the top matching image\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image, print_top=True\n",
    ")  # Print citation details for the top matching image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvwZIgD84CNc"
   },
   "source": [
    "The ability to identify similar text and images based on user input, powered by Gemini and embeddings, forms a crucial foundation for development of multimodal RAG systems, which you explore in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUnsv5Co6pJF"
   },
   "source": [
    "### Comparative reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AFbqHiz5vvo"
   },
   "source": [
    "Next, let's apply what you have done so far in doing comparative reasoning.\n",
    "\n",
    "For this example:\n",
    "\n",
    "* **Step 1:** You will search all the images for a specific query\n",
    "\n",
    "* **Step 2:** Send those images to Gemini 1.5 Pro to ask multiple questions, where it has to compare among those images and provide you with answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6AHCSwojyX0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_results_image_query_1 = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=\"Show me images in the documents of a Doctor or Pharmacist speaking directly to a patient\",\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding # mm_embedding_from_img_only text_embedding_from_image_description\n",
    "    image_emb=False,  # Use text embedding instead of image embedding\n",
    "    top_n=3,\n",
    "    embedding_size=1408,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsThmcQ2fitG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Matched Images\n",
    "# You can access the other two matched images using:\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_query_1[0][\"img_path\"],\n",
    "        matching_results_image_query_1[1][\"img_path\"],\n",
    "        matching_results_image_query_1[2][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zQAitoLb7Lb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points.\n",
    "Question:\n",
    " - What are the differences in settings? Are all clinical settings?\n",
    " - What the differences in average ages across all?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH9W-ib7cHNO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.5 Pro\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=[\n",
    "            prompt,\n",
    "            \"Images:\",\n",
    "            matching_results_image_query_1[0][\"image_object\"],\n",
    "            matching_results_image_query_1[1][\"image_object\"],\n",
    "            matching_results_image_query_1[2][\"image_object\"],\n",
    "        ],\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efJPPrzRhvIT"
   },
   "source": [
    "## Multimodal retrieval augmented generation (RAG)\n",
    "\n",
    "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
    "\n",
    "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
    "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
    "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
    "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
    "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
    "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62Hzuw_0_b"
   },
   "source": [
    "### Step 1: User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvTKFwOPHLQ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this time we are not passing any images, but just a simple text query.\n",
    "\n",
    "query = \"\"\"\\\n",
    " - What is the price of Loratadine 10 mg?\n",
    " - What is the standard Part B premium amount in 2024?\n",
    " - What is the Medicare number for John L Smith?\n",
    " - What conditions are CVS Specialty services available for?\n",
    " - What is the price of Vick's Nyquil Liquicap - 16 CT?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUqlkKUaYvZA"
   },
   "source": [
    "### Step 2: Get all relevant text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r65yBb5gR_NG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks of text based on the query\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=30,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIgXgVIpYzxj"
   },
   "source": [
    "### Step 3: Get all relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzu5Gf4yR_J4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all relevant images based on user query\n",
    "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=30,\n",
    "    embedding_size=1408,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhUpWlGAY2uG"
   },
   "source": [
    "### Step 4: Create context_text and context_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsnIdE05cj_1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"Task: Answer the following questions in detail one by one, providing clear reasoning and evidence from the images and text in bullet points.\n",
    "Instructions:\n",
    "\n",
    "1. **Analyze:** Carefully examine the provided images and text context.\n",
    "2. **Synthesize:** Integrate information from both the visual and textual elements.\n",
    "3. **Reason:**  Deduce logical connections and inferences to address the question.\n",
    "4. **formatting:** Please format the response as plain text, removing any unintended formatting or mathematical symbols. Ensure all characters are displayed as they appear in the original text.\n",
    "5. **Respond:** Provide a bulleted, concise, accurate answer in the following format:\n",
    "\n",
    "   * **Question:** [Question]\n",
    "   * **Answer:** [Direct response to the question]\n",
    "   * **Explanation:** [Bullet-point reasoning steps if applicable]\n",
    "   * **Source** [Image and Text citation]\n",
    "\n",
    "5. **Ambiguity:** If the context is insufficient to answer, respond \"Not enough context to answer.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# combine all the selected relevant text chunks\n",
    "context_text = [\"Text Context: \"]\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.extend(\n",
    "        [\n",
    "            \"Text Source: \",\n",
    "            f\"\"\"file_name: \"{value[\"file_name\"]}\" Page: \"{value[\"page_num\"]}\"\"\",\n",
    "            \"Text\",\n",
    "            value[\"chunk_text\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# combine all the selected relevant images\n",
    "gemini_content = [\n",
    "    instruction,\n",
    "    \"Questions: \",\n",
    "    query,\n",
    "    \"Image Context: \",\n",
    "]\n",
    "for key, value in matching_results_image_fromdescription_data.items():\n",
    "    gemini_content.extend(\n",
    "        [\n",
    "            \"Image Path: \",\n",
    "            value[\"img_path\"],\n",
    "            \"Image Description: \",\n",
    "            value[\"image_description\"],\n",
    "            \"Image:\",\n",
    "            value[\"image_object\"],\n",
    "        ]\n",
    "    )\n",
    "gemini_content.extend(context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHrtodcBAEu9"
   },
   "source": [
    "### Step 5: Pass context to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzHCi_rNjeGT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate final response using Gemini 1.5 Pro\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=gemini_content,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0FtXYl1fzKh"
   },
   "source": [
    "### Step 6: Print citations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYRLQ47or1I8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_fromdescription_data[0][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[1][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[2][\"img_path\"],\n",
    "        matching_results_image_fromdescription_data[3][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buwd_gp6HJ5K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
    "\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image_fromdescription_data, print_top=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06vYM4MOHJ1-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text citations\n",
    "\n",
    "print_text_to_text_citation(\n",
    "    matching_results_chunks_data,\n",
    "    print_top=True,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwNrHCqbi3xi"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05jynhZnkgxn"
   },
   "source": [
    "Congratulations on making it through this multimodal RAG notebook!\n",
    "\n",
    "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
    "\n",
    "* **Data dependency:** Needs high-quality paired text and visuals.\n",
    "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
    "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
    "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
    "\n",
    "\n",
    "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
